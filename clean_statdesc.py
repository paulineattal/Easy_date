# -*- coding: utf-8 -*-
"""StatDesc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hb2VmhqQRIOVHNW03JPM6w5kjun180oZ
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/M2/Projet Easy Date Python M2 SISE/train.csv", sep=";")
df.drop_duplicates()
pd.set_option("display.max_columns", 75)

#Changer les virgules en points

#print(df['int_corr'])
df['int_corr'] = df['int_corr'].str.replace(",", ".")

#print(df['int_corr'])

#print(df['zipcode'])
df['zipcode'] = df['zipcode'].str.replace(",", "")
#print(df['zipcode'])

#print(df['income'])
df['income'] = df['income'].str.replace(",", "")
#print(df["income"])

df['attr1_1'] = df['attr1_1'].str.replace(",", ".")
df['sinc1_1'] = df['sinc1_1'].str.replace(",", ".")
df['intel1_1'] = df['intel1_1'].str.replace(",", ".")
df['fun1_1'] = df['fun1_1'].str.replace(",", ".")
df['amb1_1'] = df['amb1_1'].str.replace(",", ".")
df['shar1_1'] = df['shar1_1'].str.replace(",", ".")


df['pf_o_att'] = df['pf_o_att'].str.replace(",", ".")
df['pf_o_sin'] = df['pf_o_sin'].str.replace(",", ".")
df['pf_o_int'] = df['pf_o_int'].str.replace(",", ".")
df['pf_o_fun'] = df['pf_o_fun'].str.replace(",", ".")
df['pf_o_amb'] = df['pf_o_amb'].str.replace(",", ".")
df['pf_o_sha'] = df['pf_o_sha'].str.replace(",", ".")

#Changer les str en float

df.attr1_1=df.attr1_1.astype('float64')
df.sinc1_1 = df.sinc1_1.astype('float64')
df.intel1_1 = df.intel1_1.astype('float64')
df.fun1_1 = df.fun1_1.astype('float64')
df.amb1_1 = df.amb1_1.astype('float64')
df.shar1_1 = df.shar1_1.astype('float64')

df.income = df.income.astype('float64')
df.field = df.shar1_1.astype('float64')
df.int_corr = df.int_corr.astype('float64')
df.zipcode = df.zipcode.astype('float64')

df_int = df.drop(["from", "career"], axis=1)

print(df_int.info())
sum_rates = df[["attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1", "shar1_1"]].sum(axis=1)
print(sum_rates>100)
sum_rates.plot.box()
df.int_corr.plot.box()
df.income.plot.box()

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=2, weights="distance")
df_new = pd.DataFrame(imputer.fit_transform(df_int), columns = df_int.columns)
sum_rates = df[["attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1", "shar1_1"]].sum(axis=1)
print(sum_rates>100)
sum_rates.plot.box()

df_new.to_csv("clean.csv")

import matplotlib.pyplot as plt
import numpy as np
train = df_new.copy()
pd.set_option('max_colwidth', 1000)
pd.set_option("display.max_columns",70)
train.dtypes

train.head()

train["nb"]= 1
train = train.dropna(subset=["iid","pid"])
train["pid"] = train["pid"].astype(int)
train["duo"] = np.where(train["iid"] > train["pid"], train["iid"].astype(str) + "_" + train["pid"].astype(str), train["pid"].astype(str) + "_" + train["iid"].astype(str))

train[["iid", "wave"]].drop_duplicates()["wave"].value_counts().sort_values(ascending=False)

print(train.drop_duplicates("duo")["match"].value_counts())
train.drop_duplicates("duo")["match"].value_counts(normalize=True)

print(train.drop_duplicates("iid")["gender"].value_counts())
train.drop_duplicates("iid")["gender"].value_counts(normalize=True)

dataQ4 = pd.crosstab(train.order, train.match, train.nb, aggfunc="sum", normalize='index').reset_index()
dataQ4.columns = ["order", "no", "yes"]
print(dataQ4)
from scipy.stats import pearsonr
pearsonr(dataQ4.order, dataQ4.yes)
#Plus l'ordre est grand moins il y a de match, significatif

train[["iid","sports","tvsports","exercise","dining","museums","art","hiking","gaming","clubbing",
       "reading","tv","theater","movies","concerts","music","shopping","yoga"]].groupby(["iid"]).mean().mean().sort_values(ascending=False)

dataQ7 = train[["iid","attr1_1","sinc1_1","intel1_1","fun1_1","amb1_1","shar1_1"]].replace(",",".",regex=True).astype(float).groupby("iid").mean()

corr = round(dataQ7.corr(),2)
print(corr)
pd.plotting.scatter_matrix(dataQ7, figsize=(6, 6))
plt.show()

dataQ8 = train.drop_duplicates("duo")[["int_corr","match"]]
dataQ8["int_corr"] = dataQ8["int_corr"].str.replace(",",".").astype(float)
print(dataQ8.corr())
dataQ8["int_corrPred"] = np.where(dataQ8['int_corr']>=0, 1, 0)

from sklearn import metrics
metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(dataQ8["match"], dataQ8["int_corrPred"])).plot()
plt.show()
Accuracy = metrics.accuracy_score(dataQ8["match"], dataQ8["int_corrPred"])
Precision = metrics.precision_score(dataQ8["match"], dataQ8["int_corrPred"])
Sensitivity_recall = metrics.recall_score(dataQ8["match"], dataQ8["int_corrPred"])
Specificity = metrics.recall_score(dataQ8["match"], dataQ8["int_corrPred"], pos_label=0)
F1_score = metrics.f1_score(dataQ8["match"], dataQ8["int_corrPred"])
print({"Accuracy":Accuracy,"Precision":Precision,"Sensitivity_recall":Sensitivity_recall,"Specificity":Specificity,"F1_score":F1_score})

field_txt = {1 :"Law", 2 : "Math", 3 : "Social Science, Psychologist", 4 : "Medical Science, Pharmaceuticals, and Bio Tech", 5 : "Engineering", 6 : "English/Creative Writing/ Journalism",
7 : "History/Religion/Philosophy", 8 : "Business/Econ/Finance", 9 : "Education, Academia", 10 : "Biological Sciences/Chemistry/Physics", 11 : "Social Work", 12 : "Undergrad/undecided",
13 : "Political Science/International Affairs", 14 : "Film", 15 : "Fine Arts/Arts Administration", 16 : "Languages", 17 : "Architecture", 18 : "Other"}
train.replace({"field_cd" : field_txt}).field_cd.value_counts()

#Wich race accord more importance to the race of the parter
race_txt = {1 : "Black/African American", 2 : "European/Caucasian-American", 3 : "Latino/Hispanic American", 4 : "Asian/Pacific Islander/Asian-American", 5 : "Native American", 6 : "Other"}
round(train.groupby("iid")["race","imprace"].mean().groupby("race").mean(),2).sort_values(by="imprace", ascending=False).reset_index().replace({"race" : race_txt})